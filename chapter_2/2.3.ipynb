{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELATIVE ENTROPY AND MUTUAL INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、相対エントロピーと相互情報量を導入する。相対エントロピーは一つのアルファベット上の2つの確率分布に関する距離のようなもので、相互情報量は、2つの確率変数がどの程度独立していのかを表す尺度である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def (相対エントロピー、カルバックライブラー情報量)\n",
    "\n",
    "アルファベット$\\mathcal{X}$上の任意の確率密度関数$p, q$に対して、その相対エントロピー$D(p||q)$は、次のように定義される。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D(p || q) &:= \\sum_{x \\in \\mathcal{X}}p(x) \\log \\frac{p(x)}{q(x)}\\\\\n",
    "& = E_p\\left[\\log \\frac{p(X)}{q(X)}\\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def (相互情報量)\n",
    "\n",
    "確率変数の組$X, Y$にたいして、相互情報量$I(X; Y)$は次のように定義される。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X; Y) &:= \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}}p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}\\\\\n",
    "&= D(p(x, y) || p(x)p(y)) \\\\\n",
    "& = E_p\\left[\\log \\frac{p(X, Y)}{p(X)p(Y)}\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "ここで、$p(x, y)$は$X,Y$の同時分布、$p(x), p(y)$はそれぞれ周辺分布である。つまり、\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(x) & = \\sum_{y \\in \\mathcal{Y}}p(x, y) \\\\\n",
    "p(y) & = \\sum_{x \\in \\mathcal{X}}p(x, y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "である。また、$D(p(x, y) || p(x)p(y))$の記法は標準的ではないが、相対エントロピーである。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
